# Running malware, automate conversion of information in features
# process activity
# file changes
# registry key changes
# API calls
# network activity
# memory dumps
# debugger

# Start process monitor
# Find process and dump only events from that process

# Build classifier
import os
import json
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.pipeline import Pipeline
from nltk import ngrams
import collections

def getAPICallsFromLog(logPath):
    """
    Extract API calls from the log file (assumed to be in JSON format).
    :param logPath: Path to the log file.
    :return: List of API calls.
    """
    apiCallsSequence = []
    with open(logPath, 'r') as f:
        try:
            data = json.load(f)
            api_calls_array = "[" + data['api_calls'] + "]"
            api_calls = json.loads(api_calls_array)
            for api_call in api_calls:
                call = api_call['class'+":"+api_call['method']]
                apiCallsSequence.append(call)
        except json.JSONDecodeError:
            return []
    # Assuming logs are structured with an "api_calls" field
    return apiCallsSequence

directoriesWithLabels = [("DA Logs/Benigh", 0), ("DA Logs/Malware", 1)]
corpus = []
y = []
for directory, label in directoriesWithLabels:
    files = os.listdir(directory)
    for file in files:
        filePath = directory+"/"+file
        corpus.append(getAPICallsFromLog(filePath))
        y.append(label)

print(corpus[0])

corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, y, test_size=0.33, random_state=42)

def textToNgrams(text, n):
    Ngrams = ngrams(text, n)
    return list(Ngrams)

def extractNgramsCounts(text, N):
    Ngrams = textToNgrams(text, N)
    return collections.Counter(Ngrams)

def getNGramfeaturesFromSample(file, K1_most_common_list):
    K1 = len(K1_most_common_list)
    fv = K1 * 0
    fileNgrams = extractNgramsCounts(file, N)
    for i in range(K1):
        fv[i] = fileNgrams[K1_most_common_list[i]]
    return fv

# Each API call is token/Ngram
N = 3
K1 = 2000
totalNgramCount = collections.Counter([])

for file in corpus_train:
    totalNgramCount += extractNgramsCounts(file, N)

K1_most_common = totalNgramCount.most_common(K1)
K1_most_common_list = [x[0] for x in K1_most_common]

print(K1_most_common_list)

def featurizeSample(file, Ngrams_list):
    K1 = len(Ngrams_list)
    fv = K1 * [0]
    fileNgrams = extractNgramsCounts(file, N)
    for i in range(K1):
        fv[i] = fileNgrams[Ngrams_list[i]]
    return fv

X_train = []
for sample in corpus_train:
    X_train.append(featurizeSample(sample, K1_most_common_list))

X_train = np.asarray(X_train)
X_test = []

for sample in corpus_test:
    X_test.append(featurizeSample(sample, K1_most_common_list))
X_test = np.asarray(X_test)

print(X_train.shape)
print(X_test.shape)

K2 = 1000
mi_rf_pipeline = Pipeline([('mi_selector', SelectKBest(mutual_info_classif, k=K2)), (RandomForestClassifier(n_estimators=100)),])

mi_rf_pipeline.fit(X_train, y_train)
print("Training Accuracy: ")
print(mi_rf_pipeline.score(X_train, y))
print("Tesing accuracy: ")
print(mi_rf_pipeline.score(X_test, y_test))

# Decrease the overfit